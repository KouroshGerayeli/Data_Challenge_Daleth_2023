{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfvbbTi06pk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2286ef15-84c8-4374-8461-31aeef473968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import Sequential, layers\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Rescaling\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Lambda"
      ],
      "metadata": {
        "id": "wceyJ53f6uQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the function to plot the accuracy and loss\n",
        "# Plot the training history\n",
        "def summarize_diagnostics(history):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='val')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='val')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N36ituSj6x93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your dataset folder\n",
        "data_dir = '/content/drive/MyDrive/Data_challenge/train/dataset_1'\n",
        "\n",
        "# Define the image size and batch size\n",
        "img_height, img_width = 512, 640\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "xaFqmEuk6zpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Convert grayscale to RGB\n",
        "def grayscale_to_rgb(image):\n",
        "    return tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "# Add new layers for your own dataset\n",
        "inputs = tf.keras.Input(shape=(img_height, img_width, 1))\n",
        "rgb_images = Lambda(grayscale_to_rgb)(inputs)\n",
        "x = resnet(rgb_images)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Create a new model\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Loading the data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  label_mode='categorical',\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  color_mode='grayscale'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  label_mode='categorical',\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  color_mode='grayscale'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=25,\n",
        "  batch_size=batch_size,\n",
        "  callbacks=[earlystop_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "_DmFFAqJ624L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb15331-4f6a-486c-d5a2-fa09fa05ff26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Found 200 files belonging to 2 classes.\n",
            "Using 160 files for training.\n",
            "Found 200 files belonging to 2 classes.\n",
            "Using 40 files for validation.\n",
            "Epoch 1/25\n",
            "10/10 [==============================] - ETA: 0s - loss: 72.2835 - accuracy: 0.4875"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_diagnostics(history)"
      ],
      "metadata": {
        "id": "I4gmZZ-U63lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Img_for_segmentation/resnet50_v2.h5')"
      ],
      "metadata": {
        "id": "9JSn5At57FRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}